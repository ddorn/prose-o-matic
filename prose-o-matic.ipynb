{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from main import *\n",
    "    from bpe import BPE\n",
    "except ImportError:\n",
    "    # We are on colab, we need to paste main.py in a cell below\n",
    "    pass\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Iterator, Tuple, List\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops torchtyping  # not on kaggle nor colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = Path('/extra/diego/the-pile/29.jsonl')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset(deterministic=True, max_length=1_000_000) -> Iterator[str]:\n",
    "    total_length = 0\n",
    "    while True:\n",
    "        for doc in jsonlines.open(DATASET):\n",
    "            doc = doc['text'] if isinstance(doc, dict) else doc\n",
    "            if deterministic or (not deterministic and random.random() < 0.01):\n",
    "                total_length += len(doc)\n",
    "                if total_length > max_length > 0:\n",
    "                    doc = doc[:max_length - total_length]\n",
    "                yield doc\n",
    "\n",
    "            if total_length >= max_length > 0:\n",
    "                return\n",
    "\n",
    "\n",
    "def get_text(length: int, from_=dataset(False, -1)) -> str:\n",
    "    text = ''\n",
    "    while len(text) < length:\n",
    "        text += next(from_)\n",
    "    return text[:length]\n",
    "\n",
    "sum(map(len, dataset()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE.train_from_text(dataset(), 10000, 2)\n",
    "bpe.save(input('Save to:') + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE.load('bpe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bpe.token_frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "bpe = BPE.load('bpe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time per call for different batch sizes and text sizes\n",
    "results = {}\n",
    "for batch_size in [1, 10, 100]:\n",
    "    results[batch_size] = {}\n",
    "    for text_size in range(2, 10):\n",
    "        text_size = 4**text_size\n",
    "        if text_size * batch_size > 10**6:\n",
    "            continue\n",
    "        # Start with a small number of runs\n",
    "        num_runs = 5\n",
    "        t = 0.0\n",
    "        while t < 0.2:\n",
    "            # Time the function call using the current number of runs\n",
    "            t = timeit.timeit(\"bpe.tokenize(texts)\",\n",
    "                              setup=\"texts = [get_text(text_size) for _ in range(batch_size)]\",\n",
    "                              globals=globals(),\n",
    "                              number=num_runs)\n",
    "            # Double the number of runs for the next iteration\n",
    "            num_runs *= 2\n",
    "        # Store the time and number of runs in the results dictionary\n",
    "        results[batch_size][text_size] = (t / num_runs, num_runs)\n",
    "        print(f'batch_size={batch_size}, text_size={text_size}: {t / num_runs:.4f}s per call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "for block_size, data in results.items():\n",
    "    xs = [block_size * b for b in data]\n",
    "    ys = [t for t, _ in data.values()]\n",
    "    plt.loglog(xs, ys, label=f'batch_size={block_size}')\n",
    "plt.legend()\n",
    "plt.xlabel('Total text size')\n",
    "plt.ylabel('Time per call (s)')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with gpt2 tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tokens in GPT-2 vocabulary\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokens = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' Python' in gpt_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def train(model,\n",
    "          optim,\n",
    "          data_generator: Iterator[Tuple[TT['batch', 'token', int], TT['batch', 'token', int]]],\n",
    "          batch_size: int = 32,\n",
    "          max_time: float = 60.0):\n",
    "    model.train()\n",
    "    lost = 0.0\n",
    "    start_time = time()\n",
    "    batch = 0\n",
    "    while time() - start_time < max_time:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Build the batch\n",
    "        xs, ys = zip(*(next(data_generator) for _ in range(batch_size)))\n",
    "        xs = torch.stack(xs).to(device)\n",
    "        ys = torch.stack(ys).to(device)\n",
    "        \n",
    "        loss = model.loss(xs, ys)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if lost == 0.0:\n",
    "            lost = loss.item()\n",
    "        else:\n",
    "            lost = 0.99 * lost + 0.01 * loss.item()\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Batch {batch} loss: {lost:.4f}')\n",
    "        batch += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case recover model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m depth \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m UpcasingTransformer(embedding_dim, depth\u001b[39m=\u001b[39mdepth, head_count\u001b[39m=\u001b[39mhead_count, block_size\u001b[39m=\u001b[39mblock_size)\n\u001b[0;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      8\u001b[0m optim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "block_size = 100\n",
    "head_count = 4\n",
    "depth = 4\n",
    "\n",
    "model = UpcasingTransformer(embedding_dim, depth=depth, head_count=head_count, block_size=block_size)\n",
    "model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator\n",
    "@typechecked\n",
    "def upcase_dataset(\n",
    "    block_size: int, data_source: Iterator[str]\n",
    ") -> Iterator[Tuple[TT['token', int], TT['token', int]]]:\n",
    "    for doc in data_source:\n",
    "        doc = doc.encode('utf-8')\n",
    "        # print(\"Document:\", len(doc), \"bytes\\t\", doc[:30])\n",
    "        for i in range(0, len(doc), block_size):\n",
    "            batch = doc[i:i + block_size]\n",
    "            if len(batch) < block_size:\n",
    "                batch = batch + b' ' * (block_size - len(batch))\n",
    "            \n",
    "            xs = tensor(list(batch.lower()))\n",
    "            ys = xs != tensor(list(batch))\n",
    "            yield xs, ys.long()\n",
    "\n",
    "\n",
    "class ByteTokenizer:\n",
    "    \"\"\"A simple tokenizer that encodes strings as their utf-8 byte values.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(texts: List[str], pad_to=None) -> TT['batch', 'token', int]:\n",
    "        encoded = [list(t.encode('utf-8')) for t in texts]\n",
    "        # pad to max length\n",
    "        if pad_to is None:\n",
    "            pad_to = max(len(t) for t in encoded)\n",
    "        else: \n",
    "            encoded = [t[:pad_to] for t in encoded]  # truncate if too long\n",
    "        encoded = [t + [0] * (pad_to - len(t)) for t in encoded]\n",
    "        return tensor(encoded)\n",
    "\n",
    "    @staticmethod\n",
    "    def detokenize(tokens: TT['batch', 'token', int]) -> List[str]:\n",
    "        return [bytes(t).decode('utf-8', errors='replace') for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the proportion of uppercase letters in the dataset\n",
    "total = 0\n",
    "upcase = 0\n",
    "for doc in dataset():\n",
    "    total += len(doc)\n",
    "    upcase += sum(1 for c in doc if c.isupper())\n",
    "up = upcase / total\n",
    "low = 1 - up\n",
    "\n",
    "print(\"Proportion of uppercase letters:\", up)\n",
    "print(\"Proportion of lowercase letters:\", low)\n",
    "print(\"Weight of upcase:\", 1 / up)\n",
    "print(\"Weight of lowcase:\", 1 / low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optim, upcase_dataset(block_size, dataset(False, -1)), max_time=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it works\n",
    "text = get_text(block_size)\n",
    "\n",
    "print(f\"Prompt: {text.lower()!r}\")\n",
    "print(f\"Expect: {text!r}\")\n",
    "\n",
    "out = repr(model.predict(text.lower()))\n",
    "probas = model(ByteTokenizer.tokenize([text.lower()], block_size))[0]\n",
    "\n",
    "diffs = ''.join(' ' if a == b else '^' for a, b in zip(repr(text), out))\n",
    "bad_diffs = ''.join(' u'[b.isupper()] for b in out)\n",
    "\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Difes:  {diffs}\")\n",
    "print(f\"Bad:    {bad_diffs}\")\n",
    "print(diffs.count('^'), 'differences')\n",
    "probas = probas.softmax(1)\n",
    "print(\"%.2f\" % max(probas[:,1]).item(), 'max proba')\n",
    "for flip, d in zip(probas[:,1], diffs[1:]):\n",
    "    print(f\"Flip: {flip:.2f}\", end=\"\")\n",
    "    if d == '^':\n",
    "        print(\" <---\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small-pile\n",
    "file = 'small-pile.jsonl'\n",
    "with jsonlines.open(file, 'w') as writer:\n",
    "    for doc in dataset():\n",
    "        writer.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
