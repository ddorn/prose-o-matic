{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "from bpe import BPE\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Iterator, Tuple, List\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from torch import tensor\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = Path('/extra/diego/the-pile/29.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(deterministic=True, max_length=1_000_000) -> Iterator[str]:\n",
    "    total_length = 0\n",
    "    while True:\n",
    "        for doc in jsonlines.open(DATASET):\n",
    "            doc = doc['text'] if isinstance(doc, dict) else doc\n",
    "            if deterministic or (not deterministic and random.random() < 0.01):\n",
    "                total_length += len(doc)\n",
    "                if total_length > max_length > 0:\n",
    "                    doc = doc[:max_length - total_length]\n",
    "                yield doc\n",
    "\n",
    "            if total_length >= max_length > 0:\n",
    "                return\n",
    "\n",
    "\n",
    "def get_text(length: int, from_=dataset(False, -1)) -> str:\n",
    "    text = ''\n",
    "    while len(text) < length:\n",
    "        text += next(from_)\n",
    "    return text[:length]\n",
    "\n",
    "sum(map(len, dataset()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE.train_from_text(dataset(), 10000, 2)\n",
    "bpe.save(input('Save to:') + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE.load('bpe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bpe.token_frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "bpe = BPE.load('bpe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time per call for different batch sizes and text sizes\n",
    "results = {}\n",
    "for batch_size in [1, 10, 100]:\n",
    "    results[batch_size] = {}\n",
    "    for text_size in range(2, 10):\n",
    "        text_size = 4**text_size\n",
    "        if text_size * batch_size > 10**6:\n",
    "            continue\n",
    "        # Start with a small number of runs\n",
    "        num_runs = 5\n",
    "        t = 0.0\n",
    "        while t < 0.2:\n",
    "            # Time the function call using the current number of runs\n",
    "            t = timeit.timeit(\"bpe.tokenize(texts)\",\n",
    "                              setup=\"texts = [get_text(text_size) for _ in range(batch_size)]\",\n",
    "                              globals=globals(),\n",
    "                              number=num_runs)\n",
    "            # Double the number of runs for the next iteration\n",
    "            num_runs *= 2\n",
    "        # Store the time and number of runs in the results dictionary\n",
    "        results[batch_size][text_size] = (t / num_runs, num_runs)\n",
    "        print(f'batch_size={batch_size}, text_size={text_size}: {t / num_runs:.4f}s per call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "for block_size, data in results.items():\n",
    "    xs = [block_size * b for b in data]\n",
    "    ys = [t for t, _ in data.values()]\n",
    "    plt.loglog(xs, ys, label=f'batch_size={block_size}')\n",
    "plt.legend()\n",
    "plt.xlabel('Total text size')\n",
    "plt.ylabel('Time per call (s)')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with gpt2 tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tokens in GPT-2 vocabulary\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_tokens = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' Python' in gpt_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def train(model,\n",
    "          optim,\n",
    "          data_generator: Iterator[Tuple[TT['batch', 'token', int], TT['batch', 'token', int]]],\n",
    "          batch_size: int = 32,\n",
    "          max_time: float = 60.0):\n",
    "    model.train()\n",
    "    lost = 0.0\n",
    "    start_time = time()\n",
    "    batch = 0\n",
    "    while time() - start_time < max_time:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Build the batch\n",
    "        xs, ys = zip(*(next(data_generator) for _ in range(batch_size)))\n",
    "        xs = torch.stack(xs)\n",
    "        ys = torch.stack(ys)\n",
    "        \n",
    "        loss = model.loss(xs, ys)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if lost == 0.0:\n",
    "            lost = loss.item()\n",
    "        else:\n",
    "            lost = 0.99 * lost + 0.01 * loss.item()\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Batch {batch} loss: {lost:.4f}')\n",
    "        batch += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case recover model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "block_size = 100\n",
    "head_count = 4\n",
    "depth = 4\n",
    "\n",
    "model = UpcasingTransformer(embedding_dim, depth=depth, head_count=head_count, block_size=block_size)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator\n",
    "@typechecked\n",
    "def upcase_dataset(\n",
    "    block_size: int, data_source: Iterator[str]\n",
    ") -> Iterator[Tuple[TT['token', int], TT['token', int]]]:\n",
    "    for doc in data_source:\n",
    "        doc = doc.encode('utf-8')\n",
    "        # print(\"Document:\", len(doc), \"bytes\\t\", doc[:30])\n",
    "        for i in range(0, len(doc), block_size):\n",
    "            batch = doc[i:i + block_size]\n",
    "            if len(batch) < block_size:\n",
    "                batch = batch + b' ' * (block_size - len(batch))\n",
    "            \n",
    "            xs = tensor(list(batch.lower()))\n",
    "            ys = xs != tensor(list(batch))\n",
    "            yield xs, ys.long()\n",
    "\n",
    "\n",
    "class ByteTokenizer:\n",
    "    \"\"\"A simple tokenizer that encodes strings as their utf-8 byte values.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(texts: List[str], pad_to=None) -> TT['batch', 'token', int]:\n",
    "        encoded = [list(t.encode('utf-8')) for t in texts]\n",
    "        # pad to max length\n",
    "        if pad_to is None:\n",
    "            pad_to = max(len(t) for t in encoded)\n",
    "        else: \n",
    "            encoded = [t[:pad_to] for t in encoded]  # truncate if too long\n",
    "        encoded = [t + [0] * (pad_to - len(t)) for t in encoded]\n",
    "        return tensor(encoded)\n",
    "\n",
    "    @staticmethod\n",
    "    def detokenize(tokens: TT['batch', 'token', int]) -> list[str]:\n",
    "        return [bytes(t).decode('utf-8', errors='replace') for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the proportion of uppercase letters in the dataset\n",
    "total = 0\n",
    "upcase = 0\n",
    "for doc in dataset():\n",
    "    total += len(doc)\n",
    "    upcase += sum(1 for c in doc if c.isupper())\n",
    "up = upcase / total\n",
    "low = 1 - up\n",
    "\n",
    "print(\"Proportion of uppercase letters:\", up)\n",
    "print(\"Proportion of lowercase letters:\", low)\n",
    "print(\"Weight of upcase:\", 1 / up)\n",
    "print(\"Weight of lowcase:\", 1 / low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optim, upcase_dataset(block_size, dataset(False, -1)), max_time=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it works\n",
    "text = get_text(block_size)\n",
    "\n",
    "print(f\"Prompt: {text.lower()!r}\")\n",
    "print(f\"Expect: {text!r}\")\n",
    "\n",
    "out = repr(model.predict(text.lower()))\n",
    "probas = model(ByteTokenizer.tokenize([text.lower()], block_size))[0]\n",
    "\n",
    "diffs = ''.join(' ' if a == b else '^' for a, b in zip(repr(text), out))\n",
    "bad_diffs = ''.join(' u'[b.isupper()] for b in out)\n",
    "\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Difes:  {diffs}\")\n",
    "print(f\"Bad:    {bad_diffs}\")\n",
    "print(diffs.count('^'), 'differences')\n",
    "probas = probas.softmax(1)\n",
    "print(\"%.2f\" % max(probas[:,1]).item(), 'max proba')\n",
    "for flip, d in zip(probas[:,1], diffs[1:]):\n",
    "    print(f\"Flip: {flip:.2f}\", end=\"\")\n",
    "    if d == '^':\n",
    "        print(\" <---\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small-pile\n",
    "file = 'small-pile.jsonl'\n",
    "with jsonlines.open(file, 'w') as writer:\n",
    "    for doc in dataset():\n",
    "        writer.write(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
